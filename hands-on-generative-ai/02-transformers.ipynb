{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1026,  373,  257, 3223,  290, 6388,   88]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(\"It was a dark and stormy\", return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1026) \t: It\n",
      "tensor(373) \t:  was\n",
      "tensor(257) \t:  a\n",
      "tensor(3223) \t:  dark\n",
      "tensor(290) \t:  and\n",
      "tensor(6388) \t:  storm\n",
      "tensor(88) \t: y\n"
     ]
    }
   ],
   "source": [
    "for t in input_ids[0]:\n",
    "    print(t, \"\\t:\", tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = gpt2(input_ids)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1755)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_logits = gpt2(input_ids).logits[0, -1]\n",
    "final_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' night'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(final_logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " night\n",
      " day\n",
      " evening\n",
      " morning\n",
      " afternoon\n",
      " summer\n",
      " time\n",
      " winter\n",
      " weekend\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "top10_logits = torch.topk(final_logits, 10)\n",
    "\n",
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " night     46.18%\n",
      " day       23.46%\n",
      " evening   5.87%\n",
      " morning   4.42%\n",
      " afternoon 4.11%\n",
      " summer    1.34%\n",
      " time      1.33%\n",
      " winter    1.22%\n",
      " weekend   0.39%\n",
      ",          0.38%\n"
     ]
    }
   ],
   "source": [
    "top10 = torch.topk(final_logits.softmax(dim=0), 10) \n",
    "for value, index in zip(top10.values, top10.indices): \n",
    "    print(f\"{tokenizer.decode(index):<10} {value.item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs tensor([1026,  373,  257, 3223,  290, 6388,   88])\n",
      "Output IDs tensor([[ 1026,   373,   257,  3223,   290,  6388,    88,  1755,    13,   383,\n",
      "          2344,   373, 19280,    11,   290,   262, 15114,   547,  7463,    13,\n",
      "           383,  2344,   373, 19280,    11,   290,   262]])\n",
      "Generated text: It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the\n"
     ]
    }
   ],
   "source": [
    "output_ids = gpt2.generate(input_ids, max_new_tokens=20)\n",
    "decoded_text = tokenizer.decode(output_ids[0])\n",
    "\n",
    "print(\"Input IDs\", input_ids[0]) \n",
    "print(\"Output IDs\", output_ids) \n",
    "print(f\"Generated text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night.\n",
      "\n",
      "\"It was dark and stormy,\" he said.\n",
      "\n",
      "\"It was dark and stormy,\" he said.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    num_beams=5, \n",
    "    max_new_tokens=30, \n",
    ") \n",
    "\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night.\n",
      "\n",
      "\"There was a lot of rain,\" he said. \"It was very cold.\"\n",
      "\n",
      "He said he saw a man with a gun in his hand.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beam_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    num_beams=5, \n",
    "    repetition_penalty=1.2, \n",
    "    max_new_tokens=38, \n",
    ")\n",
    " \n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy day until it broke down the big canvas on my sleep station, making me money dilapidated, and, with a big soothing mug\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed \n",
    "\n",
    "# Setting the seed ensures we get the same results every time we run this code \n",
    "set_seed(70) \n",
    "\n",
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=34, \n",
    "    top_k=0, # We'll come back to this parameter \n",
    ") \n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night, and I was alone. I was in the middle of the night, and I was suddenly awakened bygoodness, and I was thinking of the old man\n"
     ]
    }
   ],
   "source": [
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    temperature=0.4, \n",
    "    max_length=40, \n",
    "    top_k=0, \n",
    ") \n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were falling. The wind was blowing, and the clouds were\n"
     ]
    }
   ],
   "source": [
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    temperature=0.001, \n",
    "    max_length=40, \n",
    "    top_k=0, \n",
    ")\n",
    " \n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormyfleet verteutorial took possession contingt containing Carol Rhino soils titsfastKEY 07 Deaths od paradeCONT WEEK Barclays Reviskish6 EdwingarPosition serv blat Imperial licenseium Bot\n"
     ]
    }
   ],
   "source": [
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    temperature=3.0, \n",
    "    max_length=40, \n",
    "    top_k=0, \n",
    ")\n",
    " \n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night and there were some things we didn't understand. I didn't understand the nature of the problem. I was afraid and scared.\"\n",
      "\n",
      "In response to the\n"
     ]
    }
   ],
   "source": [
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=40, \n",
    "    top_k=10, \n",
    ") \n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stormy night from 7PM. Tony attended the Rangers game with his boys. He overheard them singing. He recorded a video with Jasper in the booth. He rented an empty\n"
     ]
    }
   ],
   "source": [
    "gpt2.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "sampling_output = gpt2.generate( \n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=40, \n",
    "    top_p=0.94, \n",
    "    top_k=0, \n",
    ") \n",
    "\n",
    "print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3967], [4633])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" positive\"), tokenizer.encode(\" negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(review):\n",
    "    \"\"\"Predict whether it is positive or negative\n",
    "\n",
    "    This function predicts whether a review is positive or negative\n",
    "    using a bit of clever prompting. It looks at the logits for the\n",
    "    tokens ' positive' and ' negative' (note the space before the\n",
    "    words), and returns the label with the highest score.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Question: Is the following review positive or\n",
    "negative about the movie?\n",
    "Review: {review} Answer:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  # 对提示词进行分词\n",
    "    final_logits = gpt2(input_ids).logits[0, -1]  # 从词汇中获取每个词元的logit，注意我们使用的是gpt2()而不是gpt2.generate()，因为前者返回词中的每个分词的logit，而后者仅返回所选定的分词\n",
    "    if final_logits[3967] > final_logits[4633]:  # 检测正向分词的logit是否大于负向分词的logit\n",
    "        print(\"Positive\")\n",
    "    else:\n",
    "        print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "score(\"This movie was terrible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "score(\"That was a delight to watch, 10/10 would recommend :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "score(\"A complex yet wonderful film about the depravity of man\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate English to Spanish:\n",
      "\n",
      "English: I do not speak Spanish.\n",
      "Spanish: No hablo español.\n",
      "\n",
      "English: See you later!\n",
      "Spanish: ¡Hasta luego!\n",
      "\n",
      "English: Where is a good restaurant?\n",
      "Spanish: ¿Dónde hay un buen restaurante?\n",
      "\n",
      "English: What rooms do you have available?\n",
      "Spanish: ¿Qué habitaciones tiene disponibles?\n",
      "\n",
      "English: I like soccer\n",
      "Spanish: Me gusta el fútbol\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "Translate English to Spanish:\n",
    "\n",
    "English: I do not speak Spanish.\n",
    "Spanish: No hablo español.\n",
    "\n",
    "English: See you later!\n",
    "Spanish: ¡Hasta luego!\n",
    "\n",
    "English: Where is a good restaurant?\n",
    "Spanish: ¿Dónde hay un buen restaurante?\n",
    "\n",
    "English: What rooms do you have available?\n",
    "Spanish: ¿Qué habitaciones tiene disponibles?\n",
    "\n",
    "English: I like soccer\n",
    "Spanish:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19546717405319214,\n",
       "  'token': 9841,\n",
       "  'token_str': 'dish',\n",
       "  'sequence': 'the dish is made of milk.'},\n",
       " {'score': 0.1290748566389084,\n",
       "  'token': 8808,\n",
       "  'token_str': 'cheese',\n",
       "  'sequence': 'the cheese is made of milk.'},\n",
       " {'score': 0.10590700060129166,\n",
       "  'token': 6501,\n",
       "  'token_str': 'milk',\n",
       "  'sequence': 'the milk is made of milk.'},\n",
       " {'score': 0.041120897978544235,\n",
       "  'token': 4392,\n",
       "  'token_str': 'drink',\n",
       "  'sequence': 'the drink is made of milk.'},\n",
       " {'score': 0.03712378069758415,\n",
       "  'token': 7852,\n",
       "  'token_str': 'bread',\n",
       "  'sequence': 'the bread is made of milk.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_masker = pipeline(model=\"bert-base-uncased\")\n",
    "fill_masker(\"The [MASK] is made of milk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998536109924316}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\"This movie is disgustingly good !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farmer', 'carpenter', 'gardener', 'fisherman', 'miner']\n",
      "['maid', 'nurse', 'servant', 'waitress', 'cook']\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK] during summer.\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK] during summer.\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (0.23.3)\n",
      "Requirement already satisfied: Pillow in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/envs/llm/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence_transformers\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 sentence_transformers-3.0.0 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6003]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embedding for both lists\n",
    "embedding_1 = model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
